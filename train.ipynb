{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Transfer Learning with MobileNetV3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name='1'></a>\n",
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow.keras.layers as tfl\n",
    "import tensorflow_addons as tfa\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from keras_flops import get_flops\n",
    "\n",
    "from train_utils.callbacks import LossHistory, LRCallBack\n",
    "from train_utils.metrics import TopKAccuracy\n",
    "from train_utils.utils import is_in\n",
    "from train_utils.losses import binary_focal_crossentropy_loss\n",
    "from train_utils.lite_accuracy import evaluate_model\n",
    "from train_utils.metadata_writer_for_image_classifier import generate_metadata\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "quantize_model = tfmot.quantization.keras.quantize_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "PREFETCH = 2\n",
    "IMG_SIZE = (224, 224)\n",
    "IMG_SHAPE = IMG_SIZE + (3,)\n",
    "NUM_CLASSES = 133"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3697\n",
      "FeaturesDict({\n",
      "    'image': Image(shape=(None, None, 3), dtype=tf.uint8),\n",
      "    'image/filename': Text(shape=(), dtype=tf.string),\n",
      "    'image/id': tf.int64,\n",
      "    'panoptic_image': Image(shape=(None, None, 3), dtype=tf.uint8),\n",
      "    'panoptic_image/filename': Text(shape=(), dtype=tf.string),\n",
      "    'panoptic_objects': Sequence({\n",
      "        'area': tf.int64,\n",
      "        'bbox': BBoxFeature(shape=(4,), dtype=tf.float32),\n",
      "        'id': tf.int64,\n",
      "        'is_crowd': tf.bool,\n",
      "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=133),\n",
      "    }),\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "train_dataset, train_info = tfds.load(\n",
    "    'coco/2017_panoptic',\n",
    "    split='train',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    data_dir=r'C:\\tensorflow_datasets',\n",
    "    download=True,\n",
    "    shuffle_files=True,\n",
    "    with_info=True,\n",
    ")\n",
    "\n",
    "cv_dataset, cv_info = tfds.load(\n",
    "    'coco/2017_panoptic',\n",
    "    split='validation',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    data_dir=r'C:\\tensorflow_datasets',\n",
    "    download=True,\n",
    "    shuffle_files=True,\n",
    "    with_info=True,\n",
    ")\n",
    "\n",
    "BATCHES = tf.data.experimental.cardinality(train_dataset).numpy()\n",
    "print(\n",
    "    BATCHES,\n",
    "    cv_info.features,\n",
    "    sep = '\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "batch = next(iter(train_dataset))\n",
    "print(batch['panoptic_objects']['label'])\n",
    "plt.imshow(batch['image'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_img(img):\n",
    "    processor = tf.keras.Sequential([\n",
    "        tfl.Resizing(\n",
    "            IMG_SHAPE[0],\n",
    "            IMG_SHAPE[1],\n",
    "            crop_to_aspect_ratio=True,\n",
    "        )\n",
    "    ])\n",
    "    return processor(img)\n",
    "\n",
    "def preprocess_objects(imgs, objects):\n",
    "    batch_size = imgs.shape[0]\n",
    "    y = np.zeros([batch_size, NUM_CLASSES])\n",
    "    imgs = imgs.numpy()\n",
    "    batched_bboxs, batched_labels = objects['bbox'].numpy(), objects['label'].numpy()\n",
    "    for i in range(batch_size):\n",
    "        size, bboxs, labels, = imgs[i].shape, batched_bboxs[i], batched_labels[i]\n",
    "        sw, sh = (1, size[0]/size[1]) if (size[0] > size[1]) else (size[1]/size[0], 1)\n",
    "        for bbox, label in zip(bboxs, labels):\n",
    "            ch, cw, _, _ = bbox\n",
    "            if ch * sh < 1 and cw * sw < 1:\n",
    "                y[i, label] = 1\n",
    "    return tf.constant(y, dtype=tf.float32)\n",
    "    \n",
    "def wrapper(func, inp, Tout, name=None):\n",
    "    def wrapped_func(*flat_inp):\n",
    "        reconstructed_inp = tf.nest.pack_sequence_as(inp, flat_inp,\n",
    "                                                     expand_composites=True)\n",
    "        out = func(*reconstructed_inp)\n",
    "        return out\n",
    "    flat_out = tf.py_function(\n",
    "        func=wrapped_func, \n",
    "        inp=tf.nest.flatten(inp, expand_composites=True),\n",
    "        Tout=Tout,\n",
    "        name=name)\n",
    "    return flat_out\n",
    "\n",
    "def preprocess(imgs, objects):\n",
    "    imgs = preprocess_img(imgs)\n",
    "    objects = preprocess_objects(imgs, objects)\n",
    "    return imgs, objects\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "                        lambda x: wrapper(\n",
    "                            preprocess, \n",
    "                            [x['image'], x['panoptic_objects']], \n",
    "                            (tf.float32, tf.float32)), \n",
    "                        num_parallel_calls=PREFETCH).shuffle(\n",
    "                            1024, reshuffle_each_iteration=True\n",
    "                        ).prefetch(buffer_size=PREFETCH)\n",
    "cv_dataset = cv_dataset.map(\n",
    "                        lambda x: wrapper(\n",
    "                            preprocess, \n",
    "                            [x['image'], x['panoptic_objects']], \n",
    "                            (tf.float32, tf.float32)), \n",
    "                        num_parallel_calls=PREFETCH,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## 0 to skip\n",
    "GRADIENT_CLIP = 10     # [5, 100]\n",
    "FIXED_LAYERS = 0       # [0, 276]\n",
    "LR = (-3.5, -4.5)        # [-5.5, -2.5]\n",
    "EPOCHS = 8            # [0, 30]\n",
    "BETA = 0.5             # (0, 1)   \n",
    "GAMMA = 4.0            # [2, 6]\n",
    "DROPOUT = 0.3          # [0.2, 1)\n",
    "L1, L2 = 0, 1e-4       # [0, 1e-4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PercentageModel(tf.keras.Model):\n",
    "\n",
    "  def compute_metrics(self, x, y, y_pred, sample_weight):\n",
    "    metric_results = super(PercentageModel, self).compute_metrics(\n",
    "        x, y, y_pred, sample_weight)\n",
    "    for k in metric_results:\n",
    "        metric_results[k] *= 100\n",
    "    return metric_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def mobilenet_model():\n",
    "    model = tf.keras.applications.MobileNetV3Large(\n",
    "        input_shape=IMG_SHAPE,\n",
    "        include_top=False,\n",
    "        dropout_rate=DROPOUT,\n",
    "    )\n",
    "    model.trainable = True\n",
    "    for layer in model.layers[:FIXED_LAYERS]:\n",
    "        layer.trainable = False\n",
    "    return model\n",
    "\n",
    "\n",
    "def top_model(input_shape):\n",
    "    inputs = tf.keras.Input(\n",
    "        name='Input',\n",
    "        shape=input_shape,)\n",
    "    x = inputs\n",
    "    \n",
    "    x = tf.keras.layers.GlobalAveragePooling2D(\n",
    "        name=\"GlobalAveragePool2D\")(x)\n",
    "    \n",
    "    x = tfl.Dense(\n",
    "        units=NUM_CLASSES,\n",
    "        name='Dense_0',\n",
    "        kernel_regularizer=tf.keras.regularizers.L1L2(L1, L2))(x)\n",
    "    \n",
    "    x = tf.keras.activations.sigmoid(x)\n",
    "    \n",
    "    outputs = x\n",
    "    model = tf.keras.Model(\n",
    "        inputs,\n",
    "        outputs,\n",
    "        name='Top')\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def train_models():\n",
    "    mobilenet = mobilenet_model()\n",
    "    inputs = tf.keras.Input(\n",
    "        name='Input',\n",
    "        shape=IMG_SHAPE)\n",
    "    top = top_model(\n",
    "        input_shape=mobilenet.layers[-1].output_shape[1:])\n",
    "    \n",
    "    x = inputs\n",
    "    x = mobilenet(x)\n",
    "    x = top(x)\n",
    "    \n",
    "    outputs = x\n",
    "    model = PercentageModel(\n",
    "            inputs,\n",
    "            outputs,\n",
    "            name='full',)\n",
    "    return {\n",
    "        'mobilenet': mobilenet,\n",
    "        'top': top,\n",
    "        'train': model,}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Estimate optimal learning rate"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "source": [
    "history = LossHistory(batches=200, l_r=(-5, -3))\n",
    "optimizer = tf.keras.optimizers.Adam(clipvalue=GRADIENT_CLIP)\n",
    "\n",
    "model = image_model()\n",
    "model.compile(\n",
    "    loss=crossentropy_loss,\n",
    "    optimizer=optimizer,\n",
    "    run_eagerly=True,\n",
    ")\n",
    "_ = model.fit(\n",
    "    train_dataset.take(history.batches),\n",
    "    callbacks=[history],\n",
    ")\n",
    "\n",
    "sigma = 2\n",
    "x, y = history.exp_lrs, history.losses\n",
    "dx = x[sigma:-sigma-1]\n",
    "dy = [(y[i+sigma]-y[i-sigma])/(2*sigma) for i in range(sigma, len(y)-sigma-1)]\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2)\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=x, \n",
    "        y=y,\n",
    "        mode='lines',\n",
    "        name='Loss',\n",
    "    ),\n",
    "    row=1, col=1,\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=dx, \n",
    "        y=dy,\n",
    "        mode='lines',\n",
    "        name='Loss Derivative',\n",
    "        line_color=\"#f00\"\n",
    "    ),\n",
    "    row=1, col=2,\n",
    ")\n",
    "fig.update_layout(\n",
    "    template='plotly_dark', \n",
    "    height=400, width=1000, \n",
    "    title_text='Loss',\n",
    "    xaxis_title='Log Learning Rate',\n",
    "    yaxis_title='',\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"full\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input (InputLayer)          [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " MobilenetV3large (Functiona  (None, 7, 7, 960)        2996352   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " Top (Functional)            (None, 133)               127813    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,124,165\n",
      "Trainable params: 3,099,765\n",
      "Non-trainable params: 24,400\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(clipvalue=GRADIENT_CLIP)\n",
    "\n",
    "metrics = [\n",
    "    tfa.metrics.FBetaScore(\n",
    "        num_classes=NUM_CLASSES,\n",
    "        average='weighted',\n",
    "        beta=0.1,\n",
    "        threshold=BETA,\n",
    "        name='F0.1',),\n",
    "    tf.keras.metrics.Precision(\n",
    "        thresholds=BETA,\n",
    "        name='pre',),\n",
    "    tf.keras.metrics.Recall(\n",
    "        thresholds=BETA,\n",
    "        name='rec',),]\n",
    "\n",
    "models = train_models()\n",
    "model = models['train']\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.BinaryFocalCrossentropy(GAMMA),\n",
    "    optimizer=optimizer,\n",
    "    metrics=metrics,\n",
    "    run_eagerly=True,\n",
    ")\n",
    "print(model.summary())\n",
    "\n",
    "# print('Total Flops:', get_flops(model, batch_size=BATCH_SIZE) // 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0be41139e004804a4e1ebb22c4faeb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|                                              0/8 ETA: ?s,  ?epochs/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "553f872fe4b74239ab485ef5fa6e2828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0/3697 | ETA: ? | Elapsed: 00:00                                                    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR set to: 0.000316\n"
     ]
    }
   ],
   "source": [
    "tqdm_callback = tfa.callbacks.TQDMProgressBar(\n",
    "    metrics_separator=' | ',\n",
    "    epoch_bar_format='{n_fmt}/{total_fmt} | ETA: {remaining} | Elapsed: {elapsed} {bar} {desc}',\n",
    "    metrics_format='{name}: {value:0.2f}%',\n",
    "    update_per_second=1,\n",
    ")\n",
    "\n",
    "lr_callback = LRCallBack(epochs=EPOCHS, batches=BATCHES, l_r=LR)\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=cv_dataset,\n",
    "    epochs=lr_callback.epochs,\n",
    "    verbose=0,\n",
    "    initial_epoch=0,\n",
    "    callbacks=[tqdm_callback, lr_callback],)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/x-python",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "more_epochs = 1\n",
    "\n",
    "lr_callback = LRCallBack(epochs=more_epochs, batches=BATCHES, l_r=(-6,-7))\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=cv_dataset,\n",
    "    epochs=more_epochs,\n",
    "    verbose=0,\n",
    "    initial_epoch=0,\n",
    "    callbacks=[tqdm_callback, lr_callback],)\n",
    "\n",
    "EPOCHS += more_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Plot the training and validation accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y = lr_callback.batch_losses[int(BATCHES*0.2):]\n",
    "x = np.linspace(0, len(y), len(y))\n",
    "print(x)\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        mode='lines',\n",
    "        name='Loss',\n",
    "    )\n",
    ")\n",
    "fig.update_layout(\n",
    "    template='plotly_dark', \n",
    "    height=400, width=1500, \n",
    "    title_text='Training Loss',\n",
    "    xaxis_title='Batch',\n",
    "    yaxis_title='Loss',\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "acc = history.history['F0.1']\n",
    "val_acc = history.history['val_F0.1']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "#plt.ylim([0,100])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "#plt.ylim([0,100])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.save('models\\\\main.h5')\n",
    "model.save_weights('models\\\\main_weights.h5')\n",
    "with open(\"models\\\\main_classes.txt\", 'w') as file:\n",
    "    for class_name in all_class_names:\n",
    "        file.write(class_name + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Fine Tune Quantization"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "source": [
    "model2.load_weights('models\\\\mobilenet_v3_large_food_classifier_weights.h5')\n",
    "model2.evaluate(cv_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model2)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_types = [tf.float16]\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_content=quantized_tflite_model)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "test_accuracy = evaluate_model(interpreter)\n",
    "\n",
    "print('Quant TFLite test_accuracy:', test_accuracy)\n",
    "print('Quant TF test accuracy:', q_aware_model_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "open(\"models\\\\mobilenet_v3_large_food_classifier_op.tflite\", \"wb\").write(tflite_model)\n",
    "generate_metadata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Check Network Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "image_batch, label_batch = next(iter(cv_dataset))\n",
    "plt.figure(figsize=(25, 25))\n",
    "for item in range(9):\n",
    "    ax = plt.subplot(3, 3, item + 1)\n",
    "    plt.imshow(image_batch[item].numpy().astype(\"uint8\"))\n",
    "    probs = model2(image_batch)[item]\n",
    "    label = all_class_names[tf.math.argmax(label_batch[item])]\n",
    "    pred = all_class_names[tf.math.argmax(probs)]\n",
    "    plt.title(label + '-' + pred)\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "\n",
    "test_dataset = image_dataset_from_directory(\n",
    "    'datasets\\\\test',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=IMG_SIZE,\n",
    "    label_mode='categorical',\n",
    ")\n",
    "test_labels = test_dataset.class_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "image_batch, label_batch = next(iter(test_dataset))\n",
    "plt.figure(figsize=(25, 25))\n",
    "for item in range(1):\n",
    "    ax = plt.subplot(3, 3, item + 1)\n",
    "    plt.imshow(image_batch[item].numpy().astype(\"uint8\"))\n",
    "    probs = model2(image_batch)[item]\n",
    "    print(image_batch[item])\n",
    "    label = test_labels[tf.math.argmax(label_batch[item])]\n",
    "    pred = all_class_names[tf.math.argmax(probs)]\n",
    "    plt.title(label + '-' + pred)\n",
    "    plt.axis(\"off\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "00e190cd69a94afba67d39fb62d3e2b7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "2f3a031dc1f64f4897b8299c61313c4e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "flex": "2"
      }
     },
     "35db6c1984b1464bbcf7cf6acb358221": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "377ebd3ef24444f2bac649148ed03342": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "39fb9c5de64845b7832ad3c0ee1d8aa4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "3f2092ac37c04b478c4fa178118616e5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "53d759da74414b54a23076d8317d0102": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_3f2092ac37c04b478c4fa178118616e5",
       "style": "IPY_MODEL_377ebd3ef24444f2bac649148ed03342",
       "value": "Training:   0%"
      }
     },
     "553f872fe4b74239ab485ef5fa6e2828": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_ae6ff0df7da84a2a97c20acd6cba2f8f",
        "IPY_MODEL_91d9d9e718c0438980b0e63b4146f602",
        "IPY_MODEL_754f49e15b264d699f0ccdc44cfcbdf2"
       ],
       "layout": "IPY_MODEL_feed8b5095b644f0815c3bff0f8181c1"
      }
     },
     "61789b34471f45f69be964017ac12dfb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "6aaf643070d1405eb52aa1e65a1e16d7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_cbf7364f3a7b4bb28f780c9b88905502",
       "max": 8,
       "style": "IPY_MODEL_8714f3acf89c4f8fb3f1f5b5cdb0814e"
      }
     },
     "754f49e15b264d699f0ccdc44cfcbdf2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_951ac0543db0445aa940e2c99d203966",
       "style": "IPY_MODEL_c2b17596cae34f1680ede5a39ebc26c7",
       "value": " loss: 2.33% | F0.1: 51.20% | pre: 62.28% | rec: 37.17%"
      }
     },
     "8714f3acf89c4f8fb3f1f5b5cdb0814e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "8ebb2f8861ca486b900117ee322917d2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_39fb9c5de64845b7832ad3c0ee1d8aa4",
       "style": "IPY_MODEL_61789b34471f45f69be964017ac12dfb",
       "value": " 0/8 ETA: ?s,  ?epochs/s"
      }
     },
     "91d9d9e718c0438980b0e63b4146f602": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_2f3a031dc1f64f4897b8299c61313c4e",
       "max": 3697,
       "style": "IPY_MODEL_00e190cd69a94afba67d39fb62d3e2b7",
       "value": 2189
      }
     },
     "951ac0543db0445aa940e2c99d203966": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "97c77c4aad894262b4b7acbdfb83d66a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a0be41139e004804a4e1ebb22c4faeb5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_53d759da74414b54a23076d8317d0102",
        "IPY_MODEL_6aaf643070d1405eb52aa1e65a1e16d7",
        "IPY_MODEL_8ebb2f8861ca486b900117ee322917d2"
       ],
       "layout": "IPY_MODEL_c73611b0f2c642f1bb6a77d2726d0b76"
      }
     },
     "ae6ff0df7da84a2a97c20acd6cba2f8f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_97c77c4aad894262b4b7acbdfb83d66a",
       "style": "IPY_MODEL_35db6c1984b1464bbcf7cf6acb358221",
       "value": "2189/3697 | ETA: 35:56 | Elapsed: 52:53 "
      }
     },
     "c2b17596cae34f1680ede5a39ebc26c7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "c73611b0f2c642f1bb6a77d2726d0b76": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "display": "inline-flex",
       "flex_flow": "row wrap",
       "width": "100%"
      }
     },
     "cbf7364f3a7b4bb28f780c9b88905502": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "flex": "2"
      }
     },
     "feed8b5095b644f0815c3bff0f8181c1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "display": "inline-flex",
       "flex_flow": "row wrap",
       "width": "100%"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
